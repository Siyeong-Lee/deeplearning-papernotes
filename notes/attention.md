## Show, Attend and Tell : Neural Image Caption Generation with Visual Attention

Introduces attention based model that automatically learns to describe content of images. As LSTM generates word sequence, it decides which part of image to focus.

#### key points

* Attempt to give NN ability to understand scene from image
* Introduces two attention mechanism
    * 'soft' deterministic attention mechanism trained by standard back-propagation methods
    * 'hard' stochastic attention mechanism trained by REINFORCE
* Model is made of an encoder and a decoder
    * Encoder uses CNN's lower convolutional layer to extract a vector, each representing a part of the image
    * Decoder uses LSTM to generate caption and in each step uses context vector gained from caption generated so far and vector generated by encoder
* Uses deep output layer to compute the output word probability given the LSTM state, the context vector and the previous word
* Uses pretrained VGGnet for encoder
* Dropout and early stopping for regularization strategy

#### thoughts

* Attention mechanism is awesome
* Though the idea came to mind that how would we make an ai that can generate word sequences like 'I don't know' or 'I'm confused, it looks like a cat to me'
    * Current Supervised learning forces ai to say an answer only
    * How would ai say 'I don't know' without imitating human responses?
